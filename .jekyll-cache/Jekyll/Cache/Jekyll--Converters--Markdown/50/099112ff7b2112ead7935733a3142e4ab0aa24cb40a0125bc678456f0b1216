I"„<p>About 6-8 months ago I was working on the problem of detecting out-of-distribution (OOD) samples. One of the papers I was following was using OOD samples in their loss in a clever way. They were minimizing the loss on the in distribution data (training data) and at the same time they were maximizing entropy (uncertainty) on out of distribution samples. This led to the model learning to recognize when it was shown something that does not look like the training data. For example you train a model on MNIST and show it a picture of a cat. It should throw up itsâ€™ hands and say this is not a digit. Thatâ€™s exactly what the result was.</p>

<p>This approach made me think about the following question: Can out of distribution data actually help with in distribution generalization? Hereâ€™s why I think it would. Letâ€™s say youâ€™re working on image classification and one of the classes</p>
:ET